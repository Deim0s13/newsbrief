# v0.6.1 Implementation Plan - Enhanced Clustering

**Release**: v0.6.1 - Enhanced Clustering
**Branch**: `feature/enhanced-clustering`
**Timeline**: Weeks 1-3 of Q1 2026
**Estimated Effort**: 16-20 hours
**Status**: Planning Phase

---

## ðŸŽ¯ Goals

### Primary Objectives
1. **Improve clustering accuracy** through entity extraction and semantic similarity
2. **Implement story quality scoring** to surface the best content
3. **Address critical security debt** by re-enabling security scanning
4. **Quick UX wins** for better user experience

### Success Criteria
- âœ… Entity extraction improves clustering accuracy by 20%+
- âœ… Story quality scoring surfaces important stories first
- âœ… Security scan re-enabled and passing (zero CRITICAL/HIGH vulnerabilities)
- âœ… All tests pass with new features
- âœ… No performance regression (story generation time)

---

## ðŸ“‹ Issues in This Release (6)

### Priority P0 - Critical (3 issues)
1. **#40**: Entity extraction from articles (3-4h)
2. **#41**: Semantic similarity for article clustering (4-5h)
3. **#43**: Story quality and importance scoring (2-3h)

### Priority P1 - High (1 issue)
4. **#73**: Re-enable security scanning and address vulnerabilities (2-3h)

### Priority P2 - Medium (2 issues)
5. **#67**: Improve UX when story generation returns 0 new stories (1h)
6. **#70**: Implement or remove skim/detail view toggle (1h)

---

## ðŸ”„ Implementation Order

### Phase 1: Security Foundation (Start Immediately)
**Issue #73**: Re-enable security scanning
**Effort**: 2-3 hours
**Can run in parallel**: âœ… Yes - doesn't block other work

#### Tasks
1. Review Trivy scan results
   ```bash
   docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
     aquasec/trivy:latest image newsbrief:latest
   ```

2. Fix CRITICAL vulnerabilities
   - Update base Python image if needed
   - Update vulnerable dependencies in requirements.txt
   - Document findings

3. Fix HIGH vulnerabilities
   - Similar to CRITICAL fixes
   - May require code changes if vulnerabilities are in our code

4. Remove `continue-on-error` flag
   - Edit `.github/workflows/ci-cd.yml`
   - Test that pipeline passes

5. Document security posture
   - Create `docs/security/SCAN_RESULTS.md`
   - Document any accepted risks for MEDIUM/LOW findings

**Files Modified**:
- `.github/workflows/ci-cd.yml`
- `requirements.txt` (likely)
- `Dockerfile` (possibly)

**Testing**:
- Run security scan locally
- Verify CI/CD pipeline passes
- Check for regression in functionality

**Definition of Done**:
- [ ] Security scan passes with zero CRITICAL/HIGH vulnerabilities
- [ ] `continue-on-error` removed from ci-cd.yml
- [ ] All tests still pass
- [ ] Findings documented

---

### Phase 2: Entity Extraction Foundation (Core Intelligence)
**Issue #40**: Entity extraction from articles
**Effort**: 3-4 hours
**Dependencies**: None

#### Tasks

##### 2.1 Design Entity Extraction (30 min)
- Define entity schema
  ```python
  @dataclass
  class ExtractedEntities:
      companies: List[str]      # e.g., ["Google", "OpenAI"]
      products: List[str]        # e.g., ["Gemini 2.0", "GPT-4"]
      people: List[str]          # e.g., ["Sundar Pichai"]
      technologies: List[str]    # e.g., ["AI", "Machine Learning"]
      locations: List[str]       # e.g., ["San Francisco"]
  ```

- Design LLM prompt for extraction
  ```
  Extract key entities from this article.

  Title: {title}
  Summary: {summary}

  Return JSON:
  {
    "companies": [...],
    "products": [...],
    "people": [...],
    "technologies": [...],
    "locations": [...]
  }

  Only include entities that are clearly central to the article.
  ```

##### 2.2 Implement Entity Extractor (1.5h)
- Create `app/entities.py`
  ```python
  def extract_entities(
      title: str,
      summary: str,
      model: str = "llama3.1:8b"
  ) -> ExtractedEntities:
      """Extract named entities using LLM."""
      pass

  def get_entity_overlap(
      entities1: ExtractedEntities,
      entities2: ExtractedEntities
  ) -> float:
      """Calculate overlap score between entity sets."""
      pass
  ```

- Add entity caching (avoid redundant LLM calls)
  ```python
  def get_cached_entities(article_id: int) -> Optional[ExtractedEntities]:
      """Get cached entities for an article."""
      pass
  ```

##### 2.3 Database Schema Updates (30 min)
- Add entities column to items table
  ```sql
  ALTER TABLE items ADD COLUMN entities_json TEXT;
  ALTER TABLE items ADD COLUMN entities_extracted_at DATETIME;
  ALTER TABLE items ADD COLUMN entities_model TEXT;
  ```

- Or create junction table (more flexible)
  ```sql
  CREATE TABLE IF NOT EXISTS article_entities (
      id INTEGER PRIMARY KEY,
      article_id INTEGER NOT NULL,
      entity_type TEXT NOT NULL,  -- 'company', 'product', etc.
      entity_name TEXT NOT NULL,
      confidence REAL DEFAULT 1.0,
      FOREIGN KEY (article_id) REFERENCES items(id),
      UNIQUE(article_id, entity_type, entity_name)
  );
  ```

**Recommendation**: Start with JSON column for simplicity, migrate to junction table later if needed.

##### 2.4 Integration with Clustering (1h)
- Update `app/stories.py` clustering logic
- Extract entities for all articles before clustering
- Use entity overlap in similarity calculation
  ```python
  def calculate_similarity(article1, article2):
      keyword_sim = jaccard_similarity(article1.keywords, article2.keywords)
      entity_sim = get_entity_overlap(article1.entities, article2.entities)

      # Weighted combination
      return 0.4 * keyword_sim + 0.6 * entity_sim
  ```

##### 2.5 Testing (30 min)
- Unit tests for entity extraction
- Test entity overlap calculation
- Test clustering with entity data
- Manual testing with real articles

**Files Created/Modified**:
- `app/entities.py` (NEW)
- `app/db.py` (add entities column)
- `app/stories.py` (integrate entities into clustering)
- `tests/test_entities.py` (NEW)
- `requirements.txt` (if adding spaCy or similar - probably not needed)

**Testing**:
```python
# Test entity extraction
def test_entity_extraction():
    title = "Google Announces Gemini 2.0"
    summary = "Google released Gemini 2.0, competing with OpenAI's GPT-4."

    entities = extract_entities(title, summary)

    assert "Google" in entities.companies
    assert "OpenAI" in entities.companies
    assert "Gemini 2.0" in entities.products
    assert "GPT-4" in entities.products

# Test entity overlap
def test_entity_overlap():
    entities1 = ExtractedEntities(companies=["Google", "Microsoft"], ...)
    entities2 = ExtractedEntities(companies=["Google", "Apple"], ...)

    overlap = get_entity_overlap(entities1, entities2)

    assert 0.0 <= overlap <= 1.0
    assert overlap > 0  # Google is shared
```

**Definition of Done**:
- [ ] Entity extraction function works reliably
- [ ] Entities stored in database
- [ ] Entity overlap calculation implemented
- [ ] All tests pass
- [ ] Manual testing shows improved clustering

---

### Phase 3: Semantic Similarity (Enhanced Clustering)
**Issue #41**: Semantic similarity for article clustering
**Effort**: 4-5 hours
**Dependencies**: #40 (Entity extraction)

#### Tasks

##### 3.1 Design Similarity Algorithm (1h)
- Research similarity approaches:
  - TF-IDF (classic, fast)
  - Cosine similarity on word vectors
  - BM25 (ranking algorithm)
  - Jaccard + entities (current + #40)

- **Recommendation**: Enhanced Jaccard + Entity Overlap
  ```python
  def calculate_similarity(article1, article2):
      # Text similarity (titles + summaries)
      text1 = f"{article1.title} {article1.summary}".lower()
      text2 = f"{article2.title} {article2.summary}".lower()

      # Tokenize and get keywords
      keywords1 = extract_keywords(text1)
      keywords2 = extract_keywords(text2)

      # Keyword overlap (Jaccard)
      keyword_sim = len(keywords1 & keywords2) / len(keywords1 | keywords2)

      # Entity overlap (from #40)
      entity_sim = get_entity_overlap(article1.entities, article2.entities)

      # Topic match bonus
      topic_bonus = 0.2 if article1.topic == article2.topic else 0.0

      # Weighted combination
      final_sim = (
          0.3 * keyword_sim +    # Text similarity
          0.5 * entity_sim +      # Entity overlap (most important)
          0.2 * topic_bonus       # Topic alignment
      )

      return final_sim
  ```

##### 3.2 Implement Keyword Extraction (1h)
- Enhance existing keyword extraction
- Remove stop words more aggressively
- Add bigrams/trigrams for better matching
  ```python
  def extract_keywords(text: str, n=2) -> Set[str]:
      """Extract keywords and n-grams from text."""
      # Tokenize
      words = tokenize(text)

      # Remove stop words
      words = [w for w in words if w not in STOP_WORDS]

      # Add bigrams
      bigrams = [f"{words[i]}_{words[i+1]}" for i in range(len(words)-1)]

      return set(words + bigrams)
  ```

##### 3.3 Implement Similarity Matrix (1.5h)
- Build similarity matrix for all articles
- Optimize for performance (caching, vectorization)
  ```python
  def build_similarity_matrix(articles: List[Article]) -> np.ndarray:
      """Build NÃ—N similarity matrix for articles."""
      n = len(articles)
      matrix = np.zeros((n, n))

      for i in range(n):
          for j in range(i+1, n):
              sim = calculate_similarity(articles[i], articles[j])
              matrix[i, j] = sim
              matrix[j, i] = sim

      return matrix
  ```

##### 3.4 Update Clustering Algorithm (1h)
- Use similarity matrix for clustering
- Implement threshold-based clustering
  ```python
  def cluster_articles(
      articles: List[Article],
      similarity_threshold: float = 0.4
  ) -> List[List[Article]]:
      """Cluster articles by similarity."""
      # Build similarity matrix
      sim_matrix = build_similarity_matrix(articles)

      # Cluster (simple greedy approach)
      clusters = []
      visited = set()

      for i, article in enumerate(articles):
          if i in visited:
              continue

          # Start new cluster
          cluster = [article]
          visited.add(i)

          # Find similar articles
          for j, other_article in enumerate(articles):
              if j in visited:
                  continue

              if sim_matrix[i, j] >= similarity_threshold:
                  cluster.append(other_article)
                  visited.add(j)

          # Only keep clusters with 2+ articles
          if len(cluster) >= 2:
              clusters.append(cluster)

      return clusters
  ```

##### 3.5 Performance Optimization (30 min)
- Cache similarity calculations
- Vectorize operations where possible
- Profile and optimize bottlenecks

##### 3.6 Testing (30 min)
- Test similarity calculation
- Test clustering with various thresholds
- Test edge cases (all similar, all different)
- Performance testing (100+ articles)

**Files Modified**:
- `app/stories.py` (update clustering)
- `app/models.py` (add similarity models if needed)
- `tests/test_clustering.py` (NEW or update existing)

**Testing**:
```python
def test_similarity_calculation():
    article1 = create_test_article(
        title="Google Announces AI Model",
        summary="Google released new AI capabilities..."
    )
    article2 = create_test_article(
        title="Google's New AI Release",
        summary="The tech giant unveiled AI improvements..."
    )

    similarity = calculate_similarity(article1, article2)

    assert similarity > 0.5  # Should be highly similar

def test_clustering():
    # Create test articles
    google_articles = [create_article(f"Google {i}") for i in range(3)]
    apple_articles = [create_article(f"Apple {i}") for i in range(3)]

    all_articles = google_articles + apple_articles
    clusters = cluster_articles(all_articles, threshold=0.4)

    # Should create 2 clusters
    assert len(clusters) == 2
```

**Definition of Done**:
- [ ] Similarity calculation implemented and tested
- [ ] Clustering algorithm uses semantic similarity
- [ ] Performance is acceptable (<5s for 100 articles)
- [ ] All tests pass
- [ ] Manual testing shows improved clustering over v0.5.5

---

### Phase 4: Story Quality Scoring (Ranking Intelligence)
**Issue #43**: Story quality and importance scoring
**Effort**: 2-3 hours
**Dependencies**: #40, #41 (uses entity and similarity data)

#### Tasks

##### 4.1 Design Scoring Algorithm (30 min)
- Define scoring components:
  ```python
  @dataclass
  class StoryScore:
      importance: float      # 0-1: How important is this story?
      freshness: float       # 0-1: How recent are the articles?
      source_quality: float  # 0-1: Quality of sources
      engagement: float      # 0-1: Estimated user interest

      overall: float         # Weighted combination
  ```

- Scoring factors:
  1. **Importance** (40%):
     - Number of articles in story
     - Number of high-quality sources
     - Number of unique entities
     - Topic relevance

  2. **Freshness** (30%):
     - Recency of articles (decay over time)
     - Publication spread (all at once vs over time)

  3. **Source Quality** (20%):
     - Feed health scores
     - Known authoritative sources (HN, major tech sites)

  4. **Engagement** (10%):
     - Keywords indicating "breaking" or "major"
     - Entity importance (Google vs unknown startup)

##### 4.2 Implement Importance Scoring (1h)
```python
def calculate_importance_score(story: Story) -> float:
    """Calculate story importance based on multiple factors."""

    # Article count (more articles = more important)
    article_count_score = min(len(story.articles) / 10, 1.0)  # Cap at 10

    # Source diversity (more unique sources = more important)
    unique_sources = len(set(a.feed_id for a in story.articles))
    source_diversity_score = min(unique_sources / 5, 1.0)  # Cap at 5

    # Entity richness (more entities = more important)
    all_entities = set()
    for article in story.articles:
        if article.entities:
            all_entities.update(article.entities.companies)
            all_entities.update(article.entities.products)
    entity_score = min(len(all_entities) / 10, 1.0)  # Cap at 10

    # Topic importance (some topics are more important than others)
    topic_weights = {
        "AI/ML": 1.0,
        "Security": 0.9,
        "Cloud": 0.8,
        "DevTools": 0.7,
        # ...
    }
    topic_score = topic_weights.get(story.topics[0], 0.5) if story.topics else 0.5

    # Weighted combination
    importance = (
        0.3 * article_count_score +
        0.3 * source_diversity_score +
        0.2 * entity_score +
        0.2 * topic_score
    )

    return importance
```

##### 4.3 Implement Freshness Scoring (30 min)
```python
def calculate_freshness_score(story: Story) -> float:
    """Calculate story freshness based on article ages."""
    from datetime import datetime, timedelta

    now = datetime.now()
    ages = [(now - article.published).total_seconds() / 3600
            for article in story.articles]  # Hours

    # Average age
    avg_age = sum(ages) / len(ages)

    # Exponential decay (half-life = 12 hours)
    freshness = math.exp(-avg_age / 12)

    return freshness
```

##### 4.4 Implement Source Quality Scoring (30 min)
```python
def calculate_source_quality_score(story: Story) -> float:
    """Calculate source quality based on feed health."""

    # Get feed health scores
    health_scores = [
        get_feed_health(article.feed_id)
        for article in story.articles
    ]

    # Average health
    avg_health = sum(health_scores) / len(health_scores)

    # Bonus for high-quality sources
    hq_sources = {"news.ycombinator.com", "arstechnica.com", ...}
    hq_count = sum(1 for a in story.articles if a.url in hq_sources)
    hq_bonus = min(hq_count * 0.1, 0.3)  # Max 30% bonus

    return min(avg_health + hq_bonus, 1.0)
```

##### 4.5 Combine Scores & Store (30 min)
```python
def calculate_story_score(story: Story) -> float:
    """Calculate overall story score."""

    importance = calculate_importance_score(story)
    freshness = calculate_freshness_score(story)
    source_quality = calculate_source_quality_score(story)

    # Weighted combination
    overall = (
        0.4 * importance +
        0.3 * freshness +
        0.2 * source_quality +
        0.1 * 0.5  # Engagement (placeholder for now)
    )

    # Store in database
    story.importance_score = importance
    story.freshness_score = freshness
    story.quality_score = overall

    return overall
```

##### 4.6 Update Story Generation & API (30 min)
- Calculate scores during story generation
- Sort stories by score in API responses
- Add score fields to `StoryOut` model

##### 4.7 Testing (30 min)
- Test each scoring component
- Test overall scoring
- Verify sorting works correctly

**Files Modified**:
- `app/stories.py` (add scoring functions)
- `app/models.py` (add score fields to StoryOut)
- `app/db.py` (add score columns to stories table if needed)
- `tests/test_story_scoring.py` (NEW)

**Database Schema**:
```sql
ALTER TABLE stories ADD COLUMN importance_score REAL DEFAULT 0.5;
ALTER TABLE stories ADD COLUMN freshness_score REAL DEFAULT 0.5;
ALTER TABLE stories ADD COLUMN quality_score REAL DEFAULT 0.5;
```

**Testing**:
```python
def test_importance_scoring():
    # Story with many articles from diverse sources
    big_story = create_test_story(article_count=8, unique_sources=5)

    # Story with few articles
    small_story = create_test_story(article_count=2, unique_sources=2)

    big_score = calculate_importance_score(big_story)
    small_score = calculate_importance_score(small_story)

    assert big_score > small_score

def test_freshness_scoring():
    # Recent story (1 hour old)
    recent_story = create_test_story(age_hours=1)

    # Old story (48 hours old)
    old_story = create_test_story(age_hours=48)

    recent_score = calculate_freshness_score(recent_story)
    old_score = calculate_freshness_score(old_story)

    assert recent_score > old_score
```

**Definition of Done**:
- [ ] All scoring components implemented
- [ ] Stories sorted by quality score in API
- [ ] Scores stored in database
- [ ] All tests pass
- [ ] Manual testing shows important stories ranked first

---

### Phase 5: UX Quick Wins (Polish)
**Issues #67, #70**: UX improvements
**Effort**: 2 hours total
**Can run in parallel**: âœ… Yes - independent changes

#### Task 5.1: Improve 0-Stories UX (#67)
**Effort**: 1 hour

##### Changes Needed
1. Update `app/static/js/stories.js`
   - Detect when generation returns 0 new stories
   - Show helpful message explaining why

   ```javascript
   if (data.stories_generated === 0) {
       // Check why
       if (data.message.includes("duplicate")) {
           showMessage(
               "No new stories generated",
               "All recent articles have already been included in existing stories. " +
               "Check back after feeds refresh with new content.",
               "info"
           );
       } else {
           showMessage(
               "No stories available",
               "Not enough articles to generate stories. Try refreshing feeds first.",
               "warning"
           );
       }
   }
   ```

2. Update `app/stories.py`
   - Return detailed generation stats
   ```python
   return {
       "stories_generated": len(new_stories),
       "stories_updated": len(updated_stories),
       "articles_processed": total_articles,
       "articles_used": articles_in_stories,
       "articles_duplicate": duplicate_articles,
       "message": "..." if stories_generated == 0 else "Success"
   }
   ```

**Files Modified**:
- `app/static/js/stories.js`
- `app/stories.py` (enhance return data)
- `app/templates/stories.html` (add message display area if needed)

**Testing**:
- Generate stories when all articles are already used
- Verify helpful message displays
- Test with various scenarios (no articles, all duplicates, etc.)

---

#### Task 5.2: Skim/Detail Toggle (#70)
**Effort**: 1 hour

##### Option A: Implement Toggle (Recommended)
1. Add JavaScript toggle functionality
   ```javascript
   // app/static/js/app.js
   function toggleViewMode(mode) {
       const articles = document.querySelectorAll('.article-card');

       articles.forEach(article => {
           const summary = article.querySelector('.article-summary');
           const content = article.querySelector('.article-content');

           if (mode === 'skim') {
               summary.style.display = 'block';
               if (content) content.style.display = 'none';
           } else {
               summary.style.display = 'none';
               if (content) content.style.display = 'block';
           }
       });

       // Save preference
       localStorage.setItem('viewMode', mode);
   }
   ```

2. Add event listeners
   ```javascript
   document.addEventListener('DOMContentLoaded', () => {
       const viewToggle = document.querySelector('[data-view-toggle]');
       if (viewToggle) {
           viewToggle.addEventListener('change', (e) => {
               toggleViewMode(e.target.checked ? 'detail' : 'skim');
           });

           // Restore saved preference
           const savedMode = localStorage.getItem('viewMode') || 'skim';
           if (savedMode === 'detail') {
               viewToggle.checked = true;
               toggleViewMode('detail');
           }
       }
   });
   ```

##### Option B: Remove Toggle (Alternative)
- Simply remove the toggle button from `app/templates/index.html`
- Cleaner if we decide skim mode is sufficient

**Recommendation**: Implement (Option A) - it's a nice-to-have feature

**Files Modified**:
- `app/static/js/app.js`
- `app/templates/index.html` (possibly - ensure content div exists)

**Testing**:
- Toggle between skim and detail modes
- Verify content displays correctly in each mode
- Test preference persistence (reload page)

---

## ðŸ“Š Summary Timeline

### Week 1 (Days 1-5)
- **Day 1-2**: Issue #73 (Security scanning) - Run in background
- **Day 2-3**: Issue #40 (Entity extraction)
- **Day 4-5**: Issue #41 (Semantic similarity) - Start

### Week 2 (Days 6-10)
- **Day 6**: Issue #41 (Semantic similarity) - Finish
- **Day 7-8**: Issue #43 (Story quality scoring)
- **Day 9**: Issue #67 (0-stories UX)
- **Day 10**: Issue #70 (Skim/detail toggle)

### Week 3 (Days 11-15)
- **Day 11-12**: Integration testing
- **Day 13**: Bug fixes and polish
- **Day 14**: Documentation updates
- **Day 15**: Release v0.6.1

---

## ðŸ§ª Testing Strategy

### Unit Tests
- [ ] Entity extraction tests
- [ ] Similarity calculation tests
- [ ] Clustering algorithm tests
- [ ] Story scoring tests
- [ ] Each scoring component independently

### Integration Tests
- [ ] End-to-end story generation with new features
- [ ] API endpoints return correctly scored stories
- [ ] Security scan passes in CI/CD
- [ ] No performance regression

### Manual Testing
- [ ] Generate stories from 50+ real articles
- [ ] Verify clustering quality improved
- [ ] Verify important stories ranked first
- [ ] Test UX improvements (0-stories, toggle)
- [ ] Security scan results review

### Performance Testing
- [ ] Story generation time <= v0.5.5 baseline (171s)
- [ ] Similarity calculation performance (100+ articles)
- [ ] No memory leaks with entity extraction

---

## ðŸ“ Documentation Updates

### Files to Update
- [ ] `docs/user-guide/API.md` - Add story scoring fields
- [ ] `docs/releases/v0.6.1/RELEASE_NOTES.md` - Create release notes
- [ ] `README.md` - Update features list
- [ ] `app/models.py` - Add docstrings for new scoring fields

### Documentation to Create
- [ ] `docs/development/ENTITY_EXTRACTION.md` - Entity extraction guide
- [ ] `docs/development/CLUSTERING_ALGORITHM.md` - Clustering documentation
- [ ] `docs/development/STORY_SCORING.md` - Scoring algorithm documentation
- [ ] `docs/security/SCAN_RESULTS.md` - Security scan findings

---

## âš ï¸ Risks & Mitigations

### Risk 1: Performance Degradation
**Issue**: Entity extraction + semantic similarity may slow down story generation

**Mitigation**:
- Cache entity extractions aggressively
- Profile performance at each step
- Set performance budgets (max 200s total generation time)
- Optimize if needed before release

---

### Risk 2: LLM Hallucinations in Entity Extraction
**Issue**: LLM may extract incorrect or hallucinated entities

**Mitigation**:
- Add validation to entity extraction
- Cross-reference entities with article text
- Set confidence thresholds
- Manual review of entity extraction on test data

---

### Risk 3: Security Vulnerabilities Can't Be Fixed Easily
**Issue**: Some vulnerabilities may be in dependencies with no updates

**Mitigation**:
- Document accepted risks
- Set `continue-on-error: false` only for CRITICAL/HIGH
- Keep MEDIUM/LOW as warnings (log but don't fail)
- Create follow-up issues for complex fixes

---

### Risk 4: Clustering Algorithm Changes Break Existing Stories
**Issue**: New clustering may create different story groupings

**Mitigation**:
- Archive existing stories before deploying
- Run side-by-side comparison (old vs new clustering)
- Add configuration flag to toggle algorithm
- Gradual rollout if needed

---

## âœ… Definition of Done for v0.6.1

### Code Quality
- [ ] All 6 issues closed and verified
- [ ] All tests passing (unit + integration)
- [ ] Code reviewed (self-review minimum)
- [ ] No linter errors
- [ ] Type hints added for new code

### Functionality
- [ ] Entity extraction working reliably
- [ ] Semantic similarity improves clustering
- [ ] Story scoring implemented and tested
- [ ] Important stories ranked first in API
- [ ] Security scan passing (CRITICAL/HIGH = 0)
- [ ] UX improvements work as expected

### Performance
- [ ] Story generation time <= 200s (max acceptable)
- [ ] No memory leaks
- [ ] API response times acceptable

### Documentation
- [ ] Release notes written
- [ ] API docs updated
- [ ] README updated
- [ ] Algorithm documentation complete

### Release
- [ ] Feature branch merged to `dev`
- [ ] Tagged as `v0.6.1`
- [ ] GitHub release created
- [ ] Changelog updated

---

## ðŸš€ Getting Started

### 1. Create Feature Branch
```bash
git checkout dev
git pull origin dev
git checkout -b feature/enhanced-clustering
git push -u origin feature/enhanced-clustering
```

### 2. Set Up Development Environment
```bash
# Ensure all dependencies installed
pip install -r requirements.txt

# Run tests to ensure baseline
pytest

# Check security scan locally (if Docker available)
docker build -t newsbrief:test .
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  aquasec/trivy:latest image newsbrief:test
```

### 3. Start with Issue #73 (Security)
This can run independently while you work on other issues:
```bash
# Review security scan
# Fix vulnerabilities
# Update ci-cd.yml
```

### 4. Implement Features in Order
Follow the phase order outlined above:
1. Security (#73) - parallel
2. Entity extraction (#40)
3. Semantic similarity (#41)
4. Story scoring (#43)
5. UX wins (#67, #70)

---

## ðŸ“ž Questions or Blockers?

If you encounter any issues:
1. Check related documentation in `docs/`
2. Review existing tests for examples
3. Ask for clarification on specific issues
4. Consider breaking down tasks further if needed

---

**Status**: Ready to Begin
**Next Action**: Create feature branch and start Issue #73
**Last Updated**: November 18, 2025
